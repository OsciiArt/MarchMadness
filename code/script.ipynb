{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9e9dca22-b608-431e-8b77-082bccbf26c3",
    "_uuid": "771498df676a38f17459b7e4d8387f0f8e67e51f"
   },
   "source": [
    "Mens Tourney Prediction Analysis\n",
    "\n",
    "I feel the following are important in determing a teams success in the tourney\n",
    "\n",
    "1) Seeding\n",
    "2) Strength of Conference\n",
    "3) Individual team statistics\n",
    "4) Experience\n",
    "5) Ability of team to win on the road\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "43bad430-365d-4080-a8b4-95b6d38ac5d0",
    "_uuid": "1502eb8556f1ccd5ace2724e4360e3efb3aef320",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from math import pi\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing, metrics,ensemble, model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0c80706-5fe1-451d-9f36-f38fa71b4e51",
    "_uuid": "692155783153697cfb3d11eed8ba794703dccb54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standard files\n",
    "\n",
    "#df_tourney = pd.read_csv('NCAATourneyCompactResults.csv')\n",
    "#df_season = pd.read_csv('RegularSeasonDetailedResults.csv')\n",
    "#df_teams = pd.read_csv('Teams.csv')\n",
    "#df_seeds = pd.read_csv('NCAATourneySeeds.csv')\n",
    "#df_conferences = pd.read_csv('Conferences.csv')\n",
    "#df_rankings = pd.read_csv('MasseyOrdinals.csv')\n",
    "#df_sample_sub = pd.read_csv('SampleSubmissionStage1.csv')\n",
    "\n",
    "#my custom file\n",
    "\n",
    "\n",
    "#df_tourney_experience = pd.read_csv('tourney_experience_senior_class.csv')\n",
    "\n",
    "# Kaggle locations\n",
    "\n",
    "df_tourney = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneyCompactResults.csv')\n",
    "df_season = pd.read_csv('../input/mens-machine-learning-competition-2018/RegularSeasonDetailedResults.csv')\n",
    "df_teams = pd.read_csv('../input/mens-machine-learning-competition-2018/Teams.csv')\n",
    "df_seeds = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneySeeds.csv')\n",
    "df_conferences = pd.read_csv('../input/mens-machine-learning-competition-2018/Conferences.csv')\n",
    "df_rankings = pd.read_csv('../input/mens-machine-learning-competition-2018/MasseyOrdinals.csv')\n",
    "df_sample_sub = pd.read_csv('../input/mens-machine-learning-competition-2018/SampleSubmissionStage1.csv')\n",
    "\n",
    "#private data file\n",
    "\n",
    "df_tourney_experience = pd.read_csv('../input/ncaa-tourney-experience/Tourney_Experience_Senior_Class.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eff73f73-9577-4e2c-937f-5c66ba349659",
    "_uuid": "8c655edc823a8f6271efe98509f6171229d341e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3c41506c-8e13-48ac-93c2-1faa9b444c1b",
    "_uuid": "e2f5b1de5c11add168bd3d90420fc168d7c7cc37",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Winning/losing Team Possesion Feature\n",
    "\n",
    "#https://www.nbastuffer.com/analytics101/possession/\n",
    "\n",
    "\n",
    "wPos = df_season.apply(lambda row: 0.96*(row.WFGA + row.WTO + 0.44*row.WFTA - row.WOR), axis=1)\n",
    "lPos = df_season.apply(lambda row: 0.96*(row.LFGA + row.LTO + 0.44*row.LFTA - row.LOR), axis=1)\n",
    "\n",
    "#two teams use almost the same number of possessions in a game\n",
    "#(plus/minus one or two - depending on how quarters end)\n",
    "#so let's just take the average\n",
    "\n",
    "df_season['Possesions'] = (wPos+lPos)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "564937b6-a34f-4a78-be84-20bf26c2cf89",
    "_uuid": "928bddecbefede6247456008a3e61d42fa390b1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c35c48ca-c97c-4d9a-8ea8-e1ccf2d6805f",
    "_uuid": "6563467a57abe854b1581ceef8c2486f2efbb073",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Name Player Impact Estimate Definition PIE measures a player's overall statistical contribution\n",
    "#against the total statistics in games they play in. PIE yields results which are\n",
    "#comparable to other advanced statistics (e.g. PER) using a simple formula.\n",
    "#Formula (PTS + FGM + FTM - FGA - FTA + DREB + (.5 * OREB) + AST + STL + (.5 * BLK) - PF - TO)\n",
    "# / (GmPTS + GmFGM + GmFTM - GmFGA - GmFTA + GmDREB + (.5 * GmOREB) + GmAST + GmSTL + (.5 * GmBLK) - GmPF - GmTO)\n",
    "\n",
    "#We will use this to measure Team Skill\n",
    "\n",
    "wtmp = df_season.apply(lambda row: row.WScore + row.WFGM + row.WFTM - row.WFGA - row.WFTA + row.WDR + 0.5*row.WOR + row.WAst +row.WStl + 0.5*row.WBlk - row.WPF - row.WTO, axis=1)\n",
    "ltmp = df_season.apply(lambda row: row.LScore + row.LFGM + row.LFTM - row.LFGA - row.LFTA + row.LDR + 0.5*row.LOR + row.LAst +row.LStl + 0.5*row.LBlk - row.LPF - row.LTO, axis=1) \n",
    "\n",
    "df_season['WPIE'] = wtmp/(wtmp + ltmp)\n",
    "df_season['LPIE'] = ltmp/(wtmp + ltmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "90c0b4b8-a6aa-4f2e-a63b-599a80b9da33",
    "_uuid": "7507449b3e29df4bd4b282ff5ea47f033cf4c1c5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Four factors statistic from the NBA\n",
    "\n",
    "#https://www.nbastuffer.com/analytics101/four-factors/\n",
    "\n",
    "\n",
    "#Effective Field Goal Percentage=(Field Goals Made) + 0.5*3P Field Goals Made))/(Field Goal Attempts)\n",
    "#you have to put the ball in the bucket eventually\n",
    "\n",
    "df_season['WeFGP'] = df_season.apply(lambda row:(row.WFGM + 0.5 * row.WFGM3) / row.WFGA, axis=1)      \n",
    "df_season['LeFGP'] = df_season.apply(lambda row:(row.LFGM + 0.5 * row.LFGM3) / row.LFGA, axis=1) \n",
    "\n",
    "#Turnover Rate= Turnovers/(Field Goal Attempts + 0.44*Free Throw Attempts + Turnovers)\n",
    "#he who doesnt turn the ball over wins games\n",
    "\n",
    "df_season['WTOR'] = df_season.apply(lambda row: row.WTO / (row.WFGA + 0.44*row.WFTA + row.WTO), axis=1)\n",
    "df_season['LTOR'] = df_season.apply(lambda row: row.LTO / (row.LFGA + 0.44*row.LFTA + row.LTO), axis=1)\n",
    "\n",
    "\n",
    "#Offensive Rebounding Percentage = (Offensive Rebounds)/[(Offensive Rebounds)+(Opponent’s Defensive Rebounds)]\n",
    "#You can win games controlling the offensive glass\n",
    "\n",
    "df_season['WORP'] = df_season.apply(lambda row: row.WOR / (row.WOR + row.LDR), axis=1)\n",
    "df_season['LORP'] = df_season.apply(lambda row: row.LOR / (row.LOR + row.WDR), axis=1)\n",
    "\n",
    "#Free Throw Rate=(Free Throws Made)/(Field Goals Attempted) or Free Throws Attempted/Field Goals Attempted\n",
    "#You got to get to the line to win close games\n",
    "\n",
    "df_season['WFTAR'] = df_season.apply(lambda row: row.WFTA / row.WFGA, axis=1)\n",
    "df_season['LFTAR'] = df_season.apply(lambda row: row.LFTA / row.LFGA, axis=1)\n",
    "\n",
    "#4 Factors is weighted as follows\n",
    "#1. Shooting (40%)\n",
    "#2. Turnovers (25%)\n",
    "#3. Rebounding (20%)\n",
    "#4. Free Throws (15%)\n",
    "\n",
    "df_season['W4Factor'] = df_season.apply(lambda row: .40*row.WeFGP + .25*row.WTOR + .20*row.WORP + .15*row.WFTAR, axis=1)\n",
    "df_season['L4Factor'] = df_season.apply(lambda row: .40*row.LeFGP + .25*row.LTOR + .20*row.LORP + .15*row.LFTAR, axis=1)                                      \n",
    "                                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5139add1-fb9c-4bf6-b896-1784f67f005d",
    "_uuid": "932ff10ca6d76165962e1d0cfe01bd2c617723f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Offensive efficiency (OffRtg) =  (Points / Possessions)\n",
    "#Every possession counts\n",
    "\n",
    "df_season['WOffRtg'] = df_season.apply(lambda row: (row.WScore / row.Possesions), axis=1)\n",
    "df_season['LOffRtg'] = df_season.apply(lambda row: (row.LScore / row.Possesions), axis=1)\n",
    "\n",
    "#Defensive efficiency (DefRtg) = (Opponent points / Opponent possessions)\n",
    "#defense wins championships\n",
    "\n",
    "df_season['WDefRtg'] = df_season.LOffRtg\n",
    "df_season['LDefRtg'] = df_season.WOffRtg\n",
    "\n",
    "                        \n",
    "#Assist Ratio : Percentage of team possessions that end in assists\n",
    "#distribute the rock - dont go isolation all the time\n",
    "\n",
    "df_season['WAstR'] = df_season.apply(lambda row: row.WAst / (row.WFGA + 0.44*row.WFTA + row.WAst + row.WTO), axis=1)\n",
    "df_season['LAstR'] = df_season.apply(lambda row: row.LAst / (row.LFGA + 0.44*row.LFTA + row.LAst + row.LTO), axis=1)\n",
    "\n",
    "\n",
    "#DREB% : Percentage of team defensive rebounds\n",
    "#control your own glass\n",
    "\n",
    "df_season['WDRP'] = df_season.apply(lambda row: row.WDR / (row.WDR + row.LOR), axis=1)\n",
    "df_season['LDRP'] = df_season.apply(lambda row: row.LDR / (row.LDR + row.WOR), axis=1) \n",
    "\n",
    "#Free Throw Percentage\n",
    "#Make your damn free throws\n",
    "\n",
    "df_season['WFTPCT'] = df_season.apply(lambda row : 0 if row.WFTA < 1 else row.WFTM / row.WFTA, axis=1)\n",
    "df_season['LFTPCT'] = df_season.apply(lambda row : 0 if row.LFTA < 1 else row.LFTM / row.LFTA, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cf3431c2-1f53-468a-ad80-e984a47604d6",
    "_uuid": "f9745e3db7ee84e5bed2261df68f2c868f027740",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season.drop(['WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF'], axis=1, inplace=True)\n",
    "df_season.drop(['LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "18e35640-d23d-46d1-978b-979382c3a98e",
    "_uuid": "0c1b574fe9b4d499c93670703f4c647cbda5552e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d2d2973-0e0b-4135-b5b1-18508078c338",
    "_uuid": "7a34fe23cb83e34c1549cc2f95abcef095c476f3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season_composite = pd.DataFrame()\n",
    "\n",
    "#This will aggregate individual games into season totals for a team\n",
    "\n",
    "#calculates wins and losses to get winning percentage\n",
    "\n",
    "df_season_composite['WINS'] = df_season['WTeamID'].groupby([df_season['Season'], df_season['WTeamID']]).count()\n",
    "df_season_composite['LOSSES'] = df_season['LTeamID'].groupby([df_season['Season'], df_season['LTeamID']]).count()\n",
    "df_season_composite['WINPCT'] = df_season_composite['WINS'] / (df_season_composite['WINS'] + df_season_composite['LOSSES'])\n",
    "\n",
    "# calculates averages for games team won\n",
    "\n",
    "df_season_composite['WPIE'] = df_season['WPIE'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WeFGP'] = df_season['WeFGP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WTOR'] = df_season['WTOR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WORP'] = df_season['WORP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WFTAR'] = df_season['WFTAR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['W4Factor'] = df_season['W4Factor'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WOffRtg'] = df_season['WOffRtg'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WDefRtg'] = df_season['WDefRtg'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WAstR'] = df_season['WAstR'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WDRP'] = df_season['WDRP'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "df_season_composite['WFTPCT'] = df_season['WFTPCT'].groupby([df_season['Season'], df_season['WTeamID']]).mean()\n",
    "\n",
    "# calculates averages for games team lost\n",
    "\n",
    "df_season_composite['LPIE'] = df_season['LPIE'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LeFGP'] = df_season['LeFGP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LTOR'] = df_season['LTOR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LORP'] = df_season['LORP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LFTAR'] = df_season['LFTAR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['L4Factor'] = df_season['L4Factor'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LOffRtg'] = df_season['LOffRtg'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LDefRtg'] = df_season['LDefRtg'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LAstR'] = df_season['LAstR'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LDRP'] = df_season['LDRP'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "df_season_composite['LFTPCT'] = df_season['LFTPCT'].groupby([df_season['Season'], df_season['LTeamID']]).mean()\n",
    "\n",
    "# calculates weighted average using winning percent to weight the statistic\n",
    "\n",
    "\n",
    "df_season_composite['PIE'] = df_season_composite['WPIE'] * df_season_composite['WINPCT'] + df_season_composite['LPIE'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['FG_PCT'] = df_season_composite['WeFGP'] * df_season_composite['WINPCT'] + df_season_composite['LeFGP'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['TURNOVER_RATE'] = df_season_composite['WTOR'] * df_season_composite['WINPCT'] + df_season_composite['LTOR'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['OFF_REB_PCT'] = df_season_composite['WORP'] * df_season_composite['WINPCT'] + df_season_composite['LORP'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['FT_RATE'] = df_season_composite['WFTAR'] * df_season_composite['WINPCT'] + df_season_composite['LFTAR'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['4FACTOR'] = df_season_composite['W4Factor'] * df_season_composite['WINPCT'] + df_season_composite['L4Factor'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['OFF_EFF'] = df_season_composite['WOffRtg'] * df_season_composite['WINPCT'] + df_season_composite['LOffRtg'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['DEF_EFF'] = df_season_composite['WDefRtg'] * df_season_composite['WINPCT'] + df_season_composite['LDefRtg'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['ASSIST_RATIO'] = df_season_composite['WAstR'] * df_season_composite['WINPCT'] + df_season_composite['LAstR'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['DEF_REB_PCT'] = df_season_composite['WDRP'] * df_season_composite['WINPCT'] + df_season_composite['LDRP'] * (1 - df_season_composite['WINPCT'])\n",
    "df_season_composite['FT_PCT'] = df_season_composite['WFTPCT'] * df_season_composite['WINPCT'] + df_season_composite['LFTPCT'] * (1 - df_season_composite['WINPCT'])\n",
    "\n",
    "df_season_composite.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "918590f7-f347-44a3-9000-c85088448ad3",
    "_uuid": "3f630088b68468bcf345a80a64397e34352d1b5a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kentucy and Witchita State went undefeated causing problems with the data since cant calculate average stats without WINPCT\n",
    "\n",
    "df_season_composite[df_season_composite['LOSSES'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e16c6bb6-31ac-4bcc-91e5-d86bcd20f875",
    "_uuid": "4c372c4f5a2a3d4c45343ee6e47e4c476d64a0db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Complete hack to fix the data\n",
    "\n",
    "df_season_composite.loc[4064,'WINPCT'] = 1\n",
    "df_season_composite.loc[4064,'LOSSES'] = 0\n",
    "df_season_composite.loc[4064,'PIE'] = df_season_composite.loc[4064,'WPIE']\n",
    "df_season_composite.loc[4064,'FG_PCT'] = df_season_composite.loc[4064,'WeFGP']\n",
    "df_season_composite.loc[4064,'TURNOVER_RATE'] = df_season_composite.loc[4064,'WTOR']\n",
    "df_season_composite.loc[4064,'OFF_REB_PCT'] = df_season_composite.loc[4064,'WORP']\n",
    "df_season_composite.loc[4064,'FT_RATE'] = df_season_composite.loc[4064,'WFTAR']\n",
    "df_season_composite.loc[4064,'4FACTOR'] = df_season_composite.loc[4064,'W4Factor']\n",
    "df_season_composite.loc[4064,'OFF_EFF'] = df_season_composite.loc[4064,'WOffRtg']\n",
    "df_season_composite.loc[4064,'DEF_EFF'] = df_season_composite.loc[4064,'WDefRtg']\n",
    "df_season_composite.loc[4064,'ASSIST_RATIO'] = df_season_composite.loc[4064,'WAstR']\n",
    "df_season_composite.loc[4064,'DEF_REB_PCT'] = df_season_composite.loc[4064,'WDRP']\n",
    "df_season_composite.loc[4064,'FT_PCT'] = df_season_composite.loc[4064,'WFTPCT']\n",
    "\n",
    "df_season_composite.loc[4211,'WINPCT'] = 1\n",
    "df_season_composite.loc[4211,'LOSSES'] = 0\n",
    "df_season_composite.loc[4211,'PIE'] = df_season_composite.loc[4211,'WPIE']\n",
    "df_season_composite.loc[4211,'FG_PCT'] = df_season_composite.loc[4211,'WeFGP']\n",
    "df_season_composite.loc[4211,'TURNOVER_RATE'] = df_season_composite.loc[4211,'WTOR']\n",
    "df_season_composite.loc[4211,'OFF_REB_PCT'] = df_season_composite.loc[4211,'WORP']\n",
    "df_season_composite.loc[4211,'FT_RATE'] = df_season_composite.loc[4211,'WFTAR']\n",
    "df_season_composite.loc[4211,'4FACTOR'] = df_season_composite.loc[4211,'W4Factor']\n",
    "df_season_composite.loc[4211,'OFF_EFF'] = df_season_composite.loc[4211,'WOffRtg']\n",
    "df_season_composite.loc[4211,'DEF_EFF'] = df_season_composite.loc[4211,'WDefRtg']\n",
    "df_season_composite.loc[4211,'ASSIST_RATIO'] = df_season_composite.loc[4211,'WAstR']\n",
    "df_season_composite.loc[4211,'DEF_REB_PCT'] = df_season_composite.loc[4211,'WDRP']\n",
    "df_season_composite.loc[4211,'FT_PCT'] = df_season_composite.loc[4211,'WFTPCT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ce27e31-9fd1-469c-a3ec-f0b29bd36030",
    "_uuid": "4b9e7f0aa9033f65e176a538724bb22483391bed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we only need the final summary stats\n",
    "\n",
    "df_season_composite.drop(['WINS','WPIE','WeFGP','WTOR','WORP','WFTAR','W4Factor','WOffRtg','WDefRtg','WAstR','WDRP','WFTPCT'], axis=1, inplace=True)\n",
    "df_season_composite.drop(['LOSSES','LPIE','LeFGP','LTOR','LORP','LFTAR','L4Factor','LOffRtg','LDefRtg','LAstR','LDRP','LFTPCT'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "42a17c65-115c-4b54-b8e2-64e403743812",
    "_uuid": "d4d80f48ced633dee87beeb0fd7340727e69d145",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_season_composite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33ccb0fe-5b3e-49b0-8c0b-611eda892139",
    "_uuid": "0acf5459ee7de145cc2dbf970577b5b64942d7f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a little housekeeping to make easier to graph correlation matrix\n",
    "\n",
    "columns = list(df_season_composite.columns.values) \n",
    "columns.pop(columns.index('WINPCT')) \n",
    "columns.append('WINPCT')\n",
    "df_season_composite = df_season_composite[columns]\n",
    "df_season_composite.rename(columns={'WTeamID':'TeamID'}, inplace=True)\n",
    "df_season_composite.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b4d1f4f-7996-4fe0-aae8-26d28b2243df",
    "_uuid": "31bbd32f525429357d2baf4e5208fdd03fda3e24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This shows we have some good predictors of winning percentage\n",
    "\n",
    "#the PIE variable is very powerfully correlated with winning percentage\n",
    "#also we can see turnovers will kill you as well as having a bad defense\n",
    "\n",
    "\n",
    "corrmatrix = df_season_composite.iloc[:, 2:].corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 7))\n",
    "sns.heatmap(corrmatrix, vmax=.8, cbar=True, annot=True, square=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8407e8ce-e567-41fd-8195-d20ce92ffab7",
    "_uuid": "fe50b561fe0c336e898c1f3ffc1ddc54abfbcab3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Strength of Schedule\n",
    "\n",
    "#We will use the RPI ranking of the teams before entering the tourney to get a measure of strength of schedule.\n",
    "\n",
    "#Rating Percentage Index (RPI) Formula=.25*(Team’s Winning Percentage)+\n",
    "#.50*(Opponents’  Average Winning Percentage)+0.25*(Opponents’ Opponents’  Average Winning Percentage)\n",
    "\n",
    "#The rating percentage index, commonly known as the RPI, is a quantity used to rank sports teams based upon\n",
    "#a team's wins and losses and its strength of schedule. It is one of the sports rating systems by which NCAA basketball,\n",
    "#baseball, softball, hockey, soccer, lacrosse, and volleyball teams are ranked.\n",
    "\n",
    "#The final pre-tournament rankings each year have a RankingDayNum of 133.\n",
    "#and can thus be used to make predictions of the games from the NCAA® tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a41c282-8d4e-4807-86d5-8f4ec27f799e",
    "_uuid": "b9f19db9c04d1d781a025d4d1a3fe87d43418995",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_RPI = df_rankings[df_rankings['SystemName'] == 'RPI']\n",
    "df_RPI_final = df_RPI[df_RPI['RankingDayNum'] == 133]\n",
    "df_RPI_final.drop(labels=['RankingDayNum', 'SystemName'], inplace=True, axis=1)\n",
    "df_RPI_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e6283608-7b3a-4e14-a3cd-fd6e6e6939eb",
    "_uuid": "13281648b91ea15241843204d2481dd27fdf5770",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get seeds of teams for all tourney games\n",
    "\n",
    "df_seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcd8642e-4bf7-45c6-bb4a-410c73d3373a",
    "_uuid": "efd6abef7638976bdf1f6e313b4cdad8cfb01300",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert string to an integer\n",
    "\n",
    "df_seeds['seed_int'] = df_seeds['Seed'].apply( lambda x : int(x[1:3]) )\n",
    "df_seeds.drop(labels=['Seed'], inplace=True, axis=1) \n",
    "df_seeds.rename(columns={'seed_int':'Seed'},inplace=True)\n",
    "df_seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b48916d6-4233-4012-bf27-69d82b5ddff2",
    "_uuid": "61ebf62d71646738c005fe7cf824e0f5257f9d22",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe of team features for all seasons\n",
    "\n",
    "#ranks only start since 2003\n",
    "\n",
    "df_seeds_final = df_seeds[df_seeds['Season'] > 2002]\n",
    "\n",
    "#2 step merge\n",
    "\n",
    "df_tourney_stage = pd.merge(left=df_seeds_final, right=df_RPI_final, how='left', on=['Season', 'TeamID'])\n",
    "df_tourney_final = pd.merge(left=df_tourney_stage, right=df_season_composite, how='left', on=['Season', 'TeamID'])\n",
    "df_tourney_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "204466b7-138d-43a0-9d3c-1429c1a0282f",
    "_uuid": "db942fb0946715b7a6fb7db3691a0033818762c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I couldnt figure out how to manipulate/calculate the way I wanted so I exported to Excel and am reimporting it back in here.\n",
    "\n",
    "#df_tourney_experience = pd.read_csv('tourney_experience_senior_class.csv')\n",
    "\n",
    "#This indicates the number of tourney games that the senior class would have played in going in to this\n",
    "#years tourney (basically games played in the prior 3 tourneys) Using it as a gage of tourney experience of the team. \n",
    "#All things being equal between two #teams the team with more experience in the tourney I feel would win the game.\n",
    "\n",
    "df_tourney_experience.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1cb3f88-3d38-4e51-b47b-a0a6f4b1ad51",
    "_uuid": "24bd20eab617fb5729b21362b7177140065f01f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function looks up the number of games for a year/team combination\n",
    "\n",
    "def get_wins(year, teamid):\n",
    "    \n",
    "    row_id = df_tourney_experience[df_tourney_experience['TeamID'] == teamid].index[0]\n",
    "    column_id = df_tourney_experience.columns.get_loc(str(year))\n",
    "    games = df_tourney_experience.iloc[row_id,column_id]\n",
    "      \n",
    "    return games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ab2dc69-afc1-4269-baf6-fb7b6592346e",
    "_uuid": "671705082037452c00db703f297961ed273425d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterates thru the dataframe to build another single column dataframe by calling the function\n",
    "\n",
    "result = []\n",
    "             \n",
    "\n",
    "for row in df_tourney_final.iterrows():\n",
    "    \n",
    "    years = (df_tourney_final['Season'])\n",
    "    teams = (df_tourney_final['TeamID'])\n",
    "    \n",
    "for i in range(len(df_tourney_final)):\n",
    "    \n",
    "    matrix = ((years[i], teams[i]))\n",
    "    result.append(get_wins(*matrix))\n",
    "    \n",
    "\n",
    "team_experience = pd.DataFrame(result, columns=['experience']) \n",
    "\n",
    "team_experience.head()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cd31a13-b499-42fe-8e2b-9e46b389ca98",
    "_uuid": "19f0e26112327e5143eb077336b95ac1fde1826e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merges them together \n",
    "\n",
    "df_tourney_final = pd.concat((df_tourney_final, team_experience), axis=1)\n",
    "\n",
    "df_tourney_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35361060-0a72-48d9-98b0-4e744ac0c248",
    "_uuid": "163f035c083f21a7cf878464021f237a58706d41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate teams in the tourney\n",
    "\n",
    "df_tourney.drop(labels=['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], inplace=True, axis=1)\n",
    "df_tourney = pd.merge(left=df_tourney, right=df_seeds, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "df_tourney = pd.merge(left=df_tourney, right=df_seeds, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\n",
    "df_tourney.drop(labels=['TeamID_x', 'TeamID_y'], inplace=True, axis=1)\n",
    "df_tourney.rename(columns={'Seed_x':'WSeed', 'Seed_y':'LSeed'},inplace=True)\n",
    "df_tourney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36361c05-b75a-4265-8b01-f6f0c57dc850",
    "_uuid": "e3fac62b6000c35193b4023263396baad65313da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Great graph showing how seeding has extreme effect in early rounds\n",
    "\n",
    "# No 16 seed has ever beaten a number 1 seed (absence of +15 values)\n",
    "# Very rarely does a #15 seed beat a #2 seed (low value of +13 values)\n",
    "\n",
    "# this needs to be in our model\n",
    "\n",
    "df_tourney['SeedDiff'] = df_tourney['WSeed'] - df_tourney['LSeed']\n",
    "sns.countplot(df_tourney['SeedDiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1fd086ce-8d09-447f-a8d7-af367093b125",
    "_uuid": "e94aca46a24064d658f7713e0fe6354ae1101ab6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quick and dirty to see how good a predictor Seed difference is\n",
    "\n",
    "df_wins = pd.DataFrame()\n",
    "df_wins['SeedDiff'] = df_tourney['SeedDiff']\n",
    "df_wins['Result'] = 1\n",
    "\n",
    "df_losses = pd.DataFrame()\n",
    "df_losses['SeedDiff'] = -df_tourney['SeedDiff']\n",
    "df_losses['Result'] = 0\n",
    "\n",
    "df_predictions = pd.concat((df_wins, df_losses))\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cce4ac54-aa97-4b0e-82d6-6d16085f66ae",
    "_uuid": "2c5367bddff5235c044c21adc5284a80559e8965",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup the data\n",
    "\n",
    "X_train = df_predictions.SeedDiff.values.reshape(-1,1)\n",
    "y_train = df_predictions.Result.values\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bf3579b5-10c1-46d7-bb71-6bd6e55a1042",
    "_uuid": "deefdb7f3aaf25c1b57ee8de8945c6577f844586",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use Logistic regression with Gridsearch for parameter tuning\n",
    "\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "params = {'C': np.logspace(start=-5, stop=3, num=9)}\n",
    "clf = GridSearchCV(logreg, params, scoring='neg_log_loss', refit=True, cv=10, )\n",
    "clf.fit(X_train, y_train)\n",
    "print('Best log_loss: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_['C']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dd9c0154-4f1e-453e-af1d-97e7686522a3",
    "_uuid": "ad8de6be973fda02ab9b9415a900664641ef35a0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model is accurately reflecting the low probability of major upsets based on seeds differentials\n",
    "\n",
    "X = np.arange(-15, 15).reshape(-1, 1)  # this creates the range of seed differentials\n",
    "preds = clf.predict_proba(X)[:,1]  # the 1 signifies winning\n",
    "\n",
    "plt.plot(X, preds)\n",
    "plt.xlabel('Team1 seed - Team2 seed')\n",
    "plt.ylabel('P(Team1 will win)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba4047c3-25f0-4ff5-8077-c738304ed209",
    "_uuid": "3f09cc7be35306fc7f228192ba046af7c63aa3c1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Seeding alone seems to predict 70% accurately\n",
    "\n",
    "train_acc = accuracy_score(y_true=y_train, y_pred=clf.predict(X_train))\n",
    "        \n",
    "print('Training Accuracy: %.2f%%' % (100 * train_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9595c8be-0f30-4597-9b87-e8f7491def36",
    "_uuid": "70574f6a2da3221746af1c4d3e0185e265d32c5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tourney.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2011a7ea-2b33-4535-8cfd-8164bea6c64a",
    "_uuid": "c7caa54199df0f34db546e18257c214f3dc42f40",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample submission file\n",
    "\n",
    "df_sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ad686ac-0df0-4516-8004-e26ac387f7b3",
    "_uuid": "65d9b165b3ef477528fa736c5f6c8ec71aad6d0d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This generates a submission file for 2014-2017 using the simple Seeds model\n",
    "\n",
    "n_test_games = len(df_sample_sub)\n",
    "\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "X_test = np.zeros(shape=(n_test_games, 1))\n",
    "\n",
    "for ii, row in df_sample_sub.iterrows():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    team1 = df_tourney_final[(df_tourney_final.TeamID == t1) & (df_tourney_final.Season == year)].Seed.values[0]\n",
    "    team2 = df_tourney_final[(df_tourney_final.TeamID == t2) & (df_tourney_final.Season == year)].Seed.values[0]\n",
    "    diff_seed = team1 - team2\n",
    "    X_test[ii, 0] = diff_seed\n",
    "\n",
    "      \n",
    "preds = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "df_sample_sub['Pred'] = preds\n",
    "\n",
    "df_sample_sub.to_csv('SeedModel.csv', index=False)\n",
    "\n",
    "df_sample_sub.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5e95301-f10e-49dc-b4c0-61479bafda41",
    "_uuid": "532bc2694e1bc65eccec300e5e4b196c0fd6c2c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tourney_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9fcaad5-5318-4ab7-a724-7e17dbb07cc0",
    "_uuid": "dc7434942f4c5eea25123a1c442b0343b82f3981",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate a list of all matchups in the tourney since 2003\n",
    "\n",
    "#df_tourney_list = pd.read_csv('NCAATourneyCompactResults.csv')\n",
    "df_tourney_list = pd.read_csv('../input/mens-machine-learning-competition-2018/NCAATourneyCompactResults.csv')\n",
    "df_tourney_list.drop(labels=['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], inplace=True, axis=1)\n",
    "df_tourney_list = df_tourney_list[df_tourney_list['Season'] > 2002]\n",
    "df_tourney_list.reset_index(inplace = True, drop=True)\n",
    "df_tourney_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d74bfbec-d600-4bb9-8aec-0888ba08c6ca",
    "_uuid": "3bace30bf6af43e2ba0381b5ae8b0024b031b0cb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets the features for the winning team\n",
    "\n",
    "df_model_winners = pd.merge(left=df_tourney_list, right=df_tourney_final ,how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "df_model_winners.drop(labels=['TeamID'], inplace=True, axis=1)\n",
    "df_model_winners.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e31ca11-db64-443a-b820-825639305a0d",
    "_uuid": "a92e860d9c2711168a53f642b4f182421e0556df",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets the features for the losing team\n",
    "\n",
    "df_model_losers = pd.merge(left=df_tourney_list, right=df_tourney_final ,how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'])\n",
    "df_model_losers.drop(labels=['TeamID'], inplace=True, axis=1)\n",
    "df_model_losers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0eed29e5-9a86-4c40-aabb-8e019b3bf334",
    "_uuid": "1396ba397efe2d71121beb51df4c8009d7a4ce51",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This generates the differences between the features between winning and losing team and assigns 1 as the classifier for winning\n",
    "\n",
    "df_model_winner_diff = (df_model_winners.iloc[:, 3:] - df_model_losers.iloc[:, 3:])\n",
    "df_model_winner_diff['result'] = 1\n",
    "df_model_winner_diff = pd.merge(left=df_model_winner_diff, right=df_tourney_list, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "#This generates the  between the features between losing and winning team and assigns 0 as the classifier for losing\n",
    "\n",
    "df_model_loser_diff = (df_model_losers.iloc[:, 3:] - df_model_winners.iloc[:, 3:])\n",
    "df_model_loser_diff['result'] = 0\n",
    "df_model_loser_diff = pd.merge(left=df_model_loser_diff, right=df_tourney_list, left_index=True, right_index=True, how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d58e59a0-25c9-40fe-b628-5cfcadd23d05",
    "_uuid": "dbdd93138ef4cc3107bb82182aafc40ef073e99f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_model_winner_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8420421e-ad8a-431c-9b48-bb00b961dcf6",
    "_uuid": "f8c714eb8f0aed1ffb01b858281060f480ac2be2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_model_loser_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae874f05-5ae5-4e8a-b92b-1707819770f2",
    "_uuid": "a086862448d44d5cda67e1c41ecdd5ad5e40416c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_predictions_tourney = pd.concat((df_model_winner_diff, df_model_loser_diff), axis=0)\n",
    "\n",
    "df_predictions_tourney.sort_values('Season', inplace=True)\n",
    "\n",
    "df_predictions_tourney.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ecb537b6-a3f8-4469-8acf-bf0be6bb001b",
    "_uuid": "ea52fb2db6a2a4dfa4b60ffa0a6966e140c08398",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predictions_tourney.head()\n",
    "df_predictions_tourney.to_csv(\"df_predictions_tourney.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c07a13f9-9808-4174-adfa-48bbc813f322",
    "_uuid": "f77bc6b545e27d1d8ebe335abcf1576fced1e871",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The plan is to test out 6 different models using Grid Search Cross Validation\n",
    "\n",
    "#1 -  Ranks  -   This will be the RPI plus Seed\n",
    "\n",
    "#2 -  Experience  - This will be the experience feature only\n",
    "\n",
    "#3 -  Stats -  This will be the seasons teams statistics features\n",
    "\n",
    "#4 -  Full -  encompassing all features of models 1, 2, and 3\n",
    "\n",
    "#5 - An ensemble model with features being the actual predictions of models 1, 2, and 3\n",
    "\n",
    "#6 - An ensemble model with features being the actual predictions of models 1, 2, and 3 plus model 4\n",
    "\n",
    "#Time to split the dataframe into its various components for modeling and testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2cd9a091-d3b8-4628-992e-f867d0b63afe",
    "_uuid": "20890464b73265ec6fafd105b3e1a66894d3b40e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test out the predictive power of the team statistic features on the training data before we begin full modeling\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "X_features_stats = df_predictions_tourney.iloc[:1426, 2:14]\n",
    "y = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2014]\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=100)\n",
    "selector.fit(X_features_stats, y)\n",
    "p_scores = (selector.pvalues_) \n",
    "F_scores = (selector.scores_)\n",
    "\n",
    "df_significance = pd.DataFrame({\"Feature\": X_features_stats.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n",
    "\n",
    "df_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9d35f632-12f6-4083-a140-caae291c9226",
    "_uuid": "cc74615e192cf959d22a139a2e22f1f5f0210db5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on the above we will drop DEF_REB_PCT and FT_PCT as their p values are > 0.05 and thus we cant rule\n",
    "# out the null hypothesis.  We will also drop FT_RATE as they doesn't seem to have much predictive power with\n",
    "# the lower F_score as well.\n",
    "\n",
    "# drop from predictions file used for training analysis\n",
    "\n",
    "df_predictions_tourney.drop(labels=['DEF_REB_PCT', 'FT_PCT', 'FT_RATE' ], inplace=True, axis=1)\n",
    "\n",
    "# drop from team statistics file used for building tourney testing analysis\n",
    "\n",
    "df_tourney_final.drop(labels=['DEF_REB_PCT', 'FT_PCT', 'FT_RATE' ], inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54a0cfd8-934f-4ba1-9ed0-8259bdb260fd",
    "_uuid": "f77a69842739849e021b43c01e9b78a2415beef0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets split the entire dataset into training/test sets and into feature categories for modeling\n",
    "\n",
    "labels = df_predictions_tourney['result']\n",
    "IDs = df_predictions_tourney.iloc[:, 13:]\n",
    "features = df_predictions_tourney.iloc[:, 0:12]                  # model 4\n",
    "features_rank = df_predictions_tourney.iloc[:, 0:2]              # model 1\n",
    "features_experience = df_predictions_tourney.iloc[:, 11:12]      # model 2\n",
    "features_stats = df_predictions_tourney.iloc[:, 2:11]            # model 3\n",
    "\n",
    "# Test data set split (2014-1017 onward which corresponds from row 1426 to the end)\n",
    "\n",
    "labels_submission = df_predictions_tourney['result'][df_predictions_tourney['Season'] > 2013]\n",
    "IDs_submission = df_predictions_tourney.iloc[1426:, 13:]\n",
    "features_submission = df_predictions_tourney.iloc[1426:,  0:12]\n",
    "\n",
    "# Training data set split (2003 thru 2013 which is from the beginning thru row 1425)\n",
    "\n",
    "y = df_predictions_tourney['result'][df_predictions_tourney['Season'] < 2014]\n",
    "IDs_training = df_predictions_tourney.iloc[:1426, 13:]\n",
    "X_features = df_predictions_tourney.iloc[:1426, 0:12]\n",
    "X_features_rank = df_predictions_tourney.iloc[:1426, 0:2]\n",
    "X_features_experience = df_predictions_tourney.iloc[:1426, 11:12]\n",
    "X_features_stats = df_predictions_tourney.iloc[:1426, 2:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "003ee9db-0f5f-4b55-bb5b-1e4c816fa5b1",
    "_uuid": "00fe6a235a8027e814aae032625ecaa7d5e04747",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test out the predictive power of the Seeding and RPI features on the training data before we begin full modeling\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=100)\n",
    "selector.fit(X_features_rank, y)\n",
    "p_scores = (selector.pvalues_) \n",
    "F_scores = (selector.scores_)\n",
    "\n",
    "df_significance = pd.DataFrame({\"Feature\": X_features_rank.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n",
    "\n",
    "df_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d901fb36-40d0-496c-980d-836555c35605",
    "_uuid": "d1ee552cbfda26a5259b16efa49cd6edf8600400",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Both ranking stats are powerful predictors so we will keep them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa6a1cc6-4ec8-45d1-b559-10c311be2fa1",
    "_uuid": "51f559ff71ed20488f2acc588fc14387b49e1ad5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets test out the predictive power of the experience feature on the training data before we begin full modeling\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=100)\n",
    "selector.fit(X_features_experience, y)\n",
    "p_scores = (selector.pvalues_) \n",
    "F_scores = (selector.scores_)\n",
    "\n",
    "df_significance = pd.DataFrame({\"Feature\":X_features_experience.columns, \"p_value\":p_scores , \"F_score\":F_scores})\n",
    "\n",
    "df_significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de60b903-2270-4fb2-a3fd-b23872d954ce",
    "_uuid": "b5dbc29746053094f88f8c413e3297ae93377ee9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experience is a factor as well. Keep it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7df776f5-f6ce-42b3-874d-25eb2343f33c",
    "_uuid": "5c72d745210c23e1c080d715493d5c0a56c1e8ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #1 (Experience)\n",
    "\n",
    "#split the training data further for cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_experience, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15d1d3b9-890c-4c07-b2f1-7a194953291a",
    "_uuid": "1c33667ef40647c5737842a6833ffa438a1a4e23",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 1 analysis\n",
    "\n",
    "# Logistic - Best score on Grid Search Cross Validation is -0.64% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.66%\n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.65%\n",
    "# RForest - Best score on Grid Search Cross Validation is -0.64%\n",
    "\n",
    "\n",
    "# Models do better than 50/50 so there is value here\n",
    "\n",
    "#We will chose Random Forest here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c3dd603f-b36c-4bcf-bad5-8db7ef2c3868",
    "_uuid": "63a4f984016cee992db80116303c166075cdc883",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #2 (Ranks)\n",
    "\n",
    "#split the training data further for cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_rank, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c20b18b-646b-400a-b958-4e950cc20362",
    "_uuid": "25e16141ea95c9117974e1d8e93450480a94624a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 2 Analysis\n",
    "\n",
    "# Logistic - Best score on Grid Search Cross Validation is -0.54% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.56%\n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.58%\n",
    "# RForest - Best score on Grid Search Cross Validation is -0.54%\n",
    "\n",
    "# Models do better than 50/50 and much better than Experience model\n",
    "\n",
    "# Logistic and RForest are very close again but RForest has a little higher accuracy\n",
    "\n",
    "# We will choose RForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6569462b-45e1-40a9-9d53-6744ba283a57",
    "_uuid": "8fdaec58b92ea76c5e3e6d295284a5712f34f443",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #3 (Stats)\n",
    "\n",
    "#split the training data further for cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_stats, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier(n_estimators=30)\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c9afeb38-dab4-4a87-ad89-902e5ddf4b7d",
    "_uuid": "601d64051df9d2ce7be6a115e24bb51de1389bca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model 3 Analysis\n",
    "\n",
    "# Logistic -Best score on Grid Search Cross Validation is -0.58% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.62%\n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.64%\n",
    "# RForest - Best score on Grid Search Cross Validation is -0.61%\n",
    "\n",
    "# Models do better than 50/50 and much better than Experience model but not quite as good as Ranks model\n",
    "\n",
    "# Logistic is the winner here.  XGBoost has high accuracy in training but way lower in test due to overfitting.\n",
    "\n",
    "# We will choose Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1798f02b-1a2f-42f0-b501-ecaf731d3ad1",
    "_uuid": "8d9f30bcf72e2cb64a37832892a1c487f5249307",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #4 (Full)\n",
    "\n",
    "#split the training data further for cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "95f9fa62-5df1-4643-afdf-8f74421fbd29",
    "_uuid": "63342f60cec7aa0524c6681a31ba95a3128f7d9f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 4 analysis\n",
    "\n",
    "# Logistic -Best score on Grid Search Cross Validation is -0.53% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.57%\n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.57%\n",
    "# RForest - Best score on Grid Search Cross Validation is -0.56%\n",
    "\n",
    "# Models do better than 50/50 and much better than Experience model\n",
    "\n",
    "# Logistic is the winner here.  XGBoost has high accuracy in training but way lower in test due to overfitting.\n",
    "\n",
    "# We will choose Logistic for this model\n",
    "\n",
    "# This model has the best performance of the 4 models so we will submit this as one of our models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "73a5bfa6-0c92-4e22-9a67-9b0f5766cd99",
    "_uuid": "5aae71f64d7667693b3adba9a1131220041455ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# steps for grabbing teams seasons info and creating the input for the Full model\n",
    "# uses the sample prediction file for the tournament with all possible games in the tourney that need to be predicted\n",
    "# the input for the model will be the difference between the teams statistics which is calculated here\n",
    "\n",
    "\n",
    "n_test_games = len(df_sample_sub)\n",
    "\n",
    "def get_year_t1_t2(ID):\n",
    "    \"\"\"Return a tuple with ints `year`, `team1` and `team2`.\"\"\"\n",
    "    return (int(x) for x in ID.split('_'))\n",
    "\n",
    "X_test = np.zeros(shape=(n_test_games, 1))\n",
    "columns = df_tourney_final.columns.get_values()\n",
    "model = []\n",
    "data = []\n",
    "\n",
    "for ii, row in df_sample_sub.iterrows():\n",
    "    year, t1, t2 = get_year_t1_t2(row.ID)\n",
    "    \n",
    "    team1 = df_tourney_final[(df_tourney_final.TeamID == t1) & (df_tourney_final.Season == year)].values\n",
    "    team2 = df_tourney_final[(df_tourney_final.TeamID == t2) & (df_tourney_final.Season == year)].values\n",
    "    \n",
    "    model = team1 - team2\n",
    "    \n",
    "    data.append(model)\n",
    "\n",
    "Predictions = pd.DataFrame(np.array(data).reshape(9112,14), columns = (columns))\n",
    "\n",
    "Predictions.drop(labels=['Season', 'TeamID'], inplace=True, axis=1)\n",
    "\n",
    "Predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b2237bcf-a79f-4093-a8dc-ba0945562ff7",
    "_uuid": "f5498630fcf6a84b9dca47be28ce169cea3ddc5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retrain the Full model using best tuned classifier on the entire scaled training data set\n",
    "\n",
    "clf = LogisticRegression(C=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X_features)\n",
    "\n",
    "clf.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5138d2a-bb45-44cc-8a8e-38934a99eeaa",
    "_uuid": "464071586695805abb7641aac220d08bad964258",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions_scaled = scaler.transform(Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68caf6f8-53d4-4e02-a4eb-50863054b8d5",
    "_uuid": "c0d0061ad6edd3ec7302f3df8ae223dc44180da2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predictions_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "279b959a-26a4-48a3-81c3-0fd1dc18db30",
    "_uuid": "9b935ce200eb2bf015c263ed49469ddd4bedfa09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the predictions for the Full Model\n",
    "\n",
    "preds = clf.predict_proba(Predictions_scaled)[:,1]\n",
    "\n",
    "df_sample_sub['Pred'] = preds\n",
    "df_sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c2cfe17-359c-4687-b429-49ce5cfa2596",
    "_uuid": "227a3e70b7f174199fd87f3c5940af8349e5667d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate prediction file\n",
    "\n",
    "df_sample_sub.to_csv('FullModel_2014_2017_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0acfefe1-f546-40a7-99da-03fc69bf91e2",
    "_uuid": "1ca438853528bf33ec107b9963011c95ee1c0a9d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble modeling\n",
    "\n",
    "# We will try some ensemble modeling by combining the 3 lesser models individually since they are modelling different\n",
    "# features that shouldnt be too corellated with each other and try combining them to see if we can beat the Full\n",
    "# models performance\n",
    "\n",
    "#from our results we will go with these 3 classifiers on the models with their respective best tuning paramaters from training\n",
    "\n",
    "#1 -  Ranks  -   Random Forest\n",
    "\n",
    "Clf_ranks = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=3,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "#2 -  Experience  -   Random Forest\n",
    "\n",
    "Clf_experience = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=3,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "#3 -  Stats -  Logistic\n",
    "\n",
    "Clf_stats = clf = LogisticRegression(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "01ed94da-0a3f-4b96-b8ff-7fa9f96b6d3b",
    "_uuid": "5601ad072acc9e985c40b7ef8a0ac4323ed25598",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup various feature training sets for individual models\n",
    "\n",
    "X_ranks = df_predictions_tourney.iloc[:1426, 0:2]\n",
    "X_experience = df_predictions_tourney.iloc[:1426, 11:12]\n",
    "X_stats = df_predictions_tourney.iloc[:1426, 2:11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4ff01f3d-51c1-455c-afa7-bf9a9112593b",
    "_uuid": "116ed49e61d3948fa8245819980ca2a501e4d86f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup the scalers.  I am scaling the entire training set here which I know is not optimal.\n",
    "\n",
    "scaler_ranks = StandardScaler()\n",
    "scaler_experience = StandardScaler()\n",
    "scaler_stats = StandardScaler()\n",
    "\n",
    "X_scaled_ranks = scaler_ranks.fit_transform(X_ranks)\n",
    "X_scaled_experience = scaler_experience.fit_transform(X_experience)\n",
    "X_scaled_stats = scaler_stats.fit_transform(X_stats)\n",
    "\n",
    "Clf_ranks.fit(X_scaled_ranks, y)\n",
    "Clf_experience.fit(X_scaled_experience, y)\n",
    "Clf_stats.fit(X_scaled_stats, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74b77456-0a02-435d-91f1-6321164c5fc3",
    "_uuid": "13f16048e34f888deda9611fc924c15afe8ca2ab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make predictions for the models \n",
    "\n",
    "pred_ranks = Clf_ranks.predict(X_scaled_ranks)\n",
    "pred_experience = Clf_experience.predict(X_scaled_experience)\n",
    "pred_stats = Clf_stats.predict(X_scaled_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3cee2984-116e-4551-b390-98e7ceaa721d",
    "_uuid": "0fb6b27c66d1215ef5a52ee1c61bcb90e3a21b99",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the 3 models predictions together \n",
    "\n",
    "pred_ranks.reshape(len(X_ranks),1)\n",
    "pred_experience.reshape(len(X_experience),1)\n",
    "pred_stats.reshape(len(X_stats),1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "328d63c0-ce71-4fc7-b006-052e859ed187",
    "_uuid": "cdc813cdd67332cd149e57be94f96c6ea170fcc2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the dataset of feature to be used for model 5\n",
    "\n",
    "model_predictions = pd.DataFrame()\n",
    "\n",
    "model_predictions = pd.DataFrame(pred_ranks, columns=['pred_ranks'] )\n",
    "model_predictions['pred_experience'] = pd.DataFrame(pred_experience, columns=['pred_experience'] )\n",
    "model_predictions['pred_stats'] = pd.DataFrame(pred_experience, columns=['pred_stats'] )\n",
    "\n",
    "model_predictions.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "063301d9-d187-4b11-a20f-490438b0a5e8",
    "_uuid": "c8eff7cd474479469552b9e71f1f66ade2b90848",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #5 (Ensemble model of weaker 3 models predictions only)\n",
    "\n",
    "#split the training data further for cross validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(model_predictions, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6c50173f-8785-4945-b1db-dac7257afa53",
    "_uuid": "f6c30c5d0f4f56fd7af043d3a0ecc49e7657f218",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model #5 Analysis\n",
    "\n",
    "# Logistic -Best score on Grid Search Cross Validation is -0.59% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.59% \n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.59% \n",
    "# RForest - Best score on Grid Search Cross Validation is -0.59% \n",
    "\n",
    "# Models do better than 50/50 but not as good as Full Model\n",
    "\n",
    "# They all had same log less and accuray as well. Very simple model? \n",
    "\n",
    "# We will not use this model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "03297471-5de3-4f43-80df-53c9a7a8ed73",
    "_uuid": "21e45f33359c045d9a7aff17cae9cc24605e0168",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets add the predictions to Full Models data to create the dataset for Model #6\n",
    "\n",
    "Full_Ensemble = pd.concat((X_features, model_predictions), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f81d8e16-92ac-443b-b05b-9e8a44c2cea9",
    "_uuid": "77ee55c20200ed95b60f071c01c7621f4160ad7b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Full_Ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7c08162-e938-4a16-b8aa-8a79d9bd9f66",
    "_uuid": "5545ffb28e18c11a501accb53d06d24e90689255",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for Model #6 (Ensemble model of model #5 predictions plus all known features)\n",
    "\n",
    "#split into train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Full_Ensemble, y, train_size=0.8, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "#Intiating Classifiers\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "\n",
    "clf3 = XGBClassifier()\n",
    "\n",
    "clf4 = DecisionTreeClassifier() \n",
    "\n",
    "clf5 = RandomForestClassifier()\n",
    "\n",
    "# Setting up the parameter grids\n",
    "\n",
    "param_grid1 = [{'clf1__C': list(np.logspace(start=-5, stop=3, num=9))}]\n",
    "\n",
    "param_grid3 = [{'learning_rate' : [0.1, 0.3],\n",
    "                'max_depth': [3, 6],\n",
    "                'min_child_weight': list(range(1, 3))}]\n",
    "\n",
    "param_grid4 = [{'max_depth': list(range(3, 6)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_leaf': [20, 50]}]\n",
    "\n",
    "param_grid5 = [{'max_depth': list(range(1, 5)),\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "                'min_samples_split' : [2, 3]}]\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('std', StandardScaler()),('clf1', clf1)])\n",
    "\n",
    "pipe3 = Pipeline([('std', StandardScaler()),('clf3', clf3)])\n",
    "\n",
    "pipe4 = Pipeline([('std', StandardScaler()),('clf4', clf4)])\n",
    "\n",
    "pipe5 = Pipeline([('std', StandardScaler()),('clf5', clf5)])\n",
    "\n",
    "\n",
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=False, random_state=2)\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid3, param_grid4, param_grid5),\n",
    "                            (pipe1, clf3, clf4, clf5,),\n",
    "                            ('Logistic', 'XGBoost', 'DTree', 'Random Forest')):\n",
    "    \n",
    "    #First loop runs GridSearch and does Cross validation to find the best parameters\n",
    "\n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='neg_log_loss',\n",
    "                       cv=outer_cv,\n",
    "                       verbose=0,\n",
    "                       refit=True,\n",
    "                       return_train_score=False)\n",
    "    \n",
    "    gcv.fit(X_train, y_train)\n",
    "    \n",
    "    gridcvs[name] = gcv\n",
    "    \n",
    "    print(name)\n",
    "    print()\n",
    "    print(gcv.best_estimator_)\n",
    "    print()\n",
    "    print('Best score on Grid Search Cross Validation is %.5f%%' % (gcv.best_score_))\n",
    "    print()\n",
    "    results = pd.DataFrame(gcv.cv_results_)\n",
    "      \n",
    "\n",
    "#Inner loop runs Cross Val Score on tuned parameter model to determine accuracy of fit        \n",
    "\n",
    "    # for name, gs_est in sorted(gridcvs.items()):\n",
    "    \n",
    "    nested_score = 0\n",
    "    nested_score = cross_val_score(gcv, \n",
    "                                  X=X_train, \n",
    "                                  y=y_train, \n",
    "                                  cv=inner_cv,\n",
    "                                  scoring='neg_log_loss')\n",
    "                                \n",
    "    \n",
    "    print('Name, Log Loss, Std Dev, based on Best Parameter Model using Cross Validation Scoring')\n",
    "    print('%s | %.2f %.2f' % (name,  nested_score.mean(),  nested_score.std() * 100,))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #Generate predictions and probabilities\n",
    "    \n",
    "    best_algo = gcv    \n",
    "\n",
    "    best_algo.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_true=y_train, y_pred=best_algo.predict(X_train))\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=best_algo.predict(X_test))\n",
    "\n",
    "    print('Training Accuracy: %.2f%%' % (100 * train_acc))\n",
    "    print('Test Accuracy: %.2f%%' % (100 * test_acc))\n",
    "    print()\n",
    "    \n",
    "    # prints classification report and confusion matrix\n",
    "    \n",
    "    predictions = best_algo.predict(X_test)\n",
    "    probability = best_algo.predict_proba(X_test)\n",
    "    print(classification_report(y_test,predictions))\n",
    "    print()\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b8da87c-04fc-422d-8675-62a46abac107",
    "_uuid": "fb87ed7181f95ec183c57a6b2e69ce3ced58c0ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model 6 Analysis\n",
    "\n",
    "# Logistic -Best score on Grid Search Cross Validation is -0.53% \n",
    "# XGBoost - Best score on Grid Search Cross Validation is -0.57%\n",
    "# DTree -   Best score on Grid Search Cross Validation is -0.56%\n",
    "# RForest - Best score on Grid Search Cross Validation is -0.55%\n",
    "\n",
    "# Models is on par with Full Model on log loss and actually a little better on Accuracy\n",
    "\n",
    "# Full Model Test Accuracy =    69.93%\n",
    "# Full Ensemble Test Accuracy =  70.28%\n",
    "\n",
    "# So lets build another submission file using this Ensemble model with Logistic Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4200d4c-f355-419f-b49f-5c9035ed7f76",
    "_uuid": "2c5b33350fea26ca3458b5c7db1ab53b63642562",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic new model DID improve slightly vs the logistic model without the 3 models predictions.  Lets go with this\n",
    "\n",
    "# Setup the test data sets for the 3 models\n",
    "\n",
    "features_rank_submission = Predictions.iloc[:, 0:2]\n",
    "features_experience_submission = Predictions.iloc[:, 11:12]\n",
    "features_stats_submission = Predictions.iloc[:,  2:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ac376787-0cf6-4a73-bcff-8d658e20caaf",
    "_uuid": "5560cdcaf554d0eb3392341b6d421720cdcf423b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make predictions on models 1,2 and 3 for the training set\n",
    "\n",
    "pred_ranks_submission = Clf_ranks.predict(features_rank_submission)\n",
    "pred_experience_submission = Clf_experience.predict(features_experience_submission)\n",
    "pred_stats_submission = Clf_stats.predict(features_stats_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54729bbd-89a2-46db-91f0-a69f39f4464f",
    "_uuid": "68c4109444c6931a355448096149adb18f46e1a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine the 3 models predictions together \n",
    "\n",
    "pred_ranks_submission.reshape(len(Predictions),1)\n",
    "pred_experience_submission.reshape(len(Predictions),1)\n",
    "pred_stats_submission.reshape(len(Predictions),1)\n",
    "\n",
    "#build into a dataframe\n",
    "\n",
    "model_predictions_submission = pd.DataFrame()\n",
    "\n",
    "model_predictions_submission = pd.DataFrame(pred_ranks_submission, columns=['pred_ranks_submission'] )\n",
    "model_predictions_submission['pred_experience'] = pd.DataFrame(pred_experience_submission, columns=['pred_experience_submission'] )\n",
    "model_predictions_submission['pred_stats'] = pd.DataFrame(pred_experience_submission, columns=['pred_stats_submission'] )\n",
    "\n",
    "model_predictions_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5be42a01-3a3a-4fe9-bffd-3a6033a28a34",
    "_uuid": "44db77ef163b9e5822aae5da7c78a493c7338267",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge with the test data set\n",
    "\n",
    "Full_Ensemble_submission = pd.concat((Predictions, model_predictions_submission), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8612726e-93b1-48ef-b74c-f0267d491a1a",
    "_uuid": "5de1b492e39af0b22f46340316aa6b1339663bc6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Full_Ensemble_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35b28d29-5076-4d35-943d-d9a6af8eb51d",
    "_uuid": "a5db75aba2beed78acf76724703a6cff463a9d06",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the final model to full training set and make predictions on scaled test set\n",
    "\n",
    "clf_Full_Ensemble = LogisticRegression(C=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(Full_Ensemble)\n",
    "\n",
    "clf_Full_Ensemble.fit(Full_Ensemble, y)\n",
    "\n",
    "Full_Ensemble_submission_scaled = scaler.transform(Full_Ensemble_submission)\n",
    "\n",
    "preds_submission = clf_Full_Ensemble.predict_proba(Full_Ensemble_submission_scaled)[:,1]\n",
    "\n",
    "df_sample_sub['Pred'] = preds_submission\n",
    "\n",
    "df_sample_sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5ea4a7e6-23b0-4244-9227-72146f3baa31",
    "_uuid": "6b0ef5bcc8ba9e743aa5681f2ad4868946b2b2f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate prediction file\n",
    "\n",
    "df_sample_sub.to_csv('Full_Ensemble_Model_2014_2017_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "264b1305-3b98-4dc5-bdfe-9723f0bcd197",
    "_uuid": "30bf1c919679c0cbedd97d5ef6b2dbfbb1209230",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bde0338e-4da1-48ef-afa7-2a9e381d5d1b",
    "_uuid": "54820916c683b447e393af1f9cc89d252846bb87",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(end_time - start_time) / 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
